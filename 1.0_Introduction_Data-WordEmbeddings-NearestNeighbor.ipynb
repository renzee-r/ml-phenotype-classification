{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/renzeer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/renzeer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time, wget, csv, string, time, random\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary, tf_embed_viz\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GaussianNoise, LSTM, Bidirectional, Dropout, Dense, Embedding, MaxPool1D, GlobalMaxPool1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pymagnitude import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "## Phenotype Classification of Electronic Health Records\n",
    "\n",
    "Electronic Health Record (EHR) data is a rapidly growing source of unstructured biomedical data. This data is extremely rich, often capturing a patient’s phenotype. In a clinical context, phenotype refers to the medical conditions, diseases, and disorders of a patient. These records can capture data in higher detail compared to structured encodings such as the International Classification of Diseases (ICD). Traditional methods for extracting phenotypes from this data typically relies on manual review or processing the data through rule-based expert systems. Both approaches are time intensive, rely heavily on human expertise, and scale poorly. This project proposes an automated approach to identifying phenotypes in EHR data through machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION\n",
    "## Word Embedding\n",
    "\n",
    "The foundation of this project is based on word embeddings, an approach that converts words into number vectors based on co-occurence. These vectors help capture word meanings and context in a format suitable for machine learning. \n",
    "\n",
    "Typically these vectors are trained on extremely large corpora, which can take a lot of time and resources. Thankfully, the word embedding space is quite mature and there exists pre-trained models, ready to use out of the box. One such model is Standford's GloVe vectors, which is trained on a corpus of 6B tokens from Wikipedia and Gigaword. These vectors are available at https://nlp.stanford.edu/projects/glove/. We will go through some exercises to explore word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "import glove_helper; reload(glove_helper)\n",
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nn_cos(v, Wv, k=10):\n",
    "    \"\"\"Find nearest neighbors of a given word, by cosine similarity.\n",
    "    \n",
    "    Returns two parallel lists: indices of nearest neighbors, and \n",
    "    their cosine similarities. Both lists are in descending order, \n",
    "    and inclusive: so nns[0] should be the index of the input word, \n",
    "    nns[1] should be the index of the first nearest neighbor, and so on.\n",
    "\n",
    "    Args:\n",
    "      v: (d-dimensional vector) word vector of interest\n",
    "      Wv: (V x d matrix) word embeddings\n",
    "      k: (int) number of neighbors to return\n",
    "    \n",
    "    Returns (nns, ds), where:\n",
    "      nns: (k-dimensional vector of int), row indices of nearest neighbors, \n",
    "        which may include the given word.\n",
    "      similarities: (k-dimensional vector of float), cosine similarity of each \n",
    "        neighbor in nns.\n",
    "    \"\"\"\n",
    "\n",
    "    v_norm = np.linalg.norm(v)\n",
    "    Wv_norm = np.linalg.norm(Wv, axis=1)\n",
    "\n",
    "    dot = np.dot(v, Wv.T)\n",
    "\n",
    "    cos_sim = dot / (v_norm * Wv_norm)\n",
    "\n",
    "    nns = np.flipud(np.argsort(cos_sim)[-k:])\n",
    "    ds = np.flipud(np.sort(cos_sim)[-k:])\n",
    "    \n",
    "    return [nns, ds]\n",
    "\n",
    "\n",
    "def show_nns(hands, word, k=10):\n",
    "    \"\"\"Helper function to print neighbors of a given word.\"\"\"\n",
    "    word = word.lower()\n",
    "    print(\"Nearest neighbors for '{:s}'\".format(word))\n",
    "    v = hands.get_vector(word)\n",
    "    for i, sim in zip(*find_nn_cos(v, hands.W, k)):\n",
    "        target_word = hands.vocab.id_to_word[i]\n",
    "        print(\"{:.03f} : '{:s}'\".format(sim, target_word))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors for 'diabetes'\n",
      "1.000 : 'diabetes'\n",
      "0.848 : 'hypertension'\n",
      "0.799 : 'obesity'\n",
      "0.780 : 'arthritis'\n",
      "0.779 : 'cancer'\n",
      "0.774 : 'alzheimer'\n",
      "0.765 : 'asthma'\n",
      "0.756 : 'cardiovascular'\n",
      "0.733 : 'disease'\n",
      "0.730 : 'epilepsy'\n",
      "\n",
      "Nearest neighbors for 'cancer'\n",
      "1.000 : 'cancer'\n",
      "0.821 : 'breast'\n",
      "0.807 : 'prostate'\n",
      "0.785 : 'disease'\n",
      "0.779 : 'diabetes'\n",
      "0.766 : 'cancers'\n",
      "0.751 : 'patients'\n",
      "0.749 : 'leukemia'\n",
      "0.744 : 'alzheimer'\n",
      "0.732 : 'lung'\n",
      "\n",
      "Nearest neighbors for 'depression'\n",
      "1.000 : 'depression'\n",
      "0.706 : 'illness'\n",
      "0.690 : 'anxiety'\n",
      "0.679 : 'severe'\n",
      "0.672 : 'onset'\n",
      "0.670 : 'schizophrenia'\n",
      "0.668 : 'disorder'\n",
      "0.666 : 'alcoholism'\n",
      "0.643 : 'psychosis'\n",
      "0.641 : 'mental'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 nearest neighbors for a few examples\n",
    "show_nns(hands, \"diabetes\")\n",
    "show_nns(hands, \"cancer\")\n",
    "show_nns(hands, \"depression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we see make sense and showcase the capability of word embeddings. However, we do run into a few issues. For one, \n",
    "loading the file into our workspace requires careful memory management. This can become a problem when dealing with larger models or when we want to tweak our models and reload the data. Another issue is that we have to build our own help functions for performing calculations on the word vectors. Not inherently an issue, but these calculations are fairly standard and it is always a good idea to work smarter, not harder.\n",
    "\n",
    "As an alternative, we can look at third-party packages that offer fast and simple support for word vector operations. The package we will use for this project is Magnitude (https://github.com/plasticityai/magnitude). This package offers \"lazy-loading for faster cold starts in development, LRU memory caching for performance in production, multiple key queries, direct featurization to the inputs for a neural network, performant similiarity calculations, and other nice to have features for edge cases like handling out-of-vocabulary keys or misspelled keys and concatenating multiple vector models together.\" These are all great features that we can leverage for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Word Vectors - Magnitude\n",
    "\n",
    "Going through a few simple comparisons and exercises, we can see the difference between working with the raw text file versus working with the magnitude file:\n",
    "  - The zip file is ~4 times larger than the magnitude file. This is even more impressive consdering the text file still needs to be unpackaged.  \n",
    "  - Load times are extremely quick for the magnitude file, far outperforming the standard file.  \n",
    "  - Querying from the standard file outperforms the magnitude file, but querying from the magnitude file is simpler and offers additional functionality.  \n",
    "  \n",
    "While the increased query times is not ideal, especially when it comes to training, the portability and the increased functionality just makes life so much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Text File:\n",
      "\tFile Size:  862182613\n",
      "\tFile Load Time:  18.083816289901733\n",
      "\tQuery Time:  0.00012183189392089844\n",
      "\tHandling out-of-vocabulary words:\n",
      "\t\tWord not found in vocabulary\n",
      "\n",
      "Magnitude File:\n",
      "\tFile Size:  266366976\n",
      "\tFile Load Time:  0.002287149429321289\n",
      "\tQuery Time:  0.006619930267333984\n",
      "\tHandling out-of-vocabulary words:\n",
      "\t\t [-0.04397694  0.08708267  0.05870734 -0.04722567 -0.03879925  0.21312321\n",
      "  0.02859145 -0.03979973 -0.02670808  0.02556176 -0.07791763  0.0055145\n",
      " -0.03020298  0.06430179 -0.00551911  0.16249717 -0.06189246 -0.12206172\n",
      " -0.02767706 -0.05265569  0.13255737  0.02846519  0.0451067   0.11242716\n",
      "  0.01290785 -0.04876954 -0.04612697 -0.03764525 -0.00251381  0.11269477\n",
      "  0.11309229  0.09421328 -0.13763386 -0.02501031  0.01126506  0.06448203\n",
      "  0.06115726 -0.12342421  0.02004041 -0.0443186  -0.02901474 -0.01431345\n",
      "  0.05068584 -0.02549015 -0.08328359 -0.07138098  0.0835982  -0.03470181\n",
      " -0.00475797 -0.07226969  0.20147627 -0.02546141  0.16691468  0.15587942\n",
      " -0.10204505 -0.28276903 -0.04359986 -0.00812922  0.26467098 -0.01318733\n",
      "  0.04115933  0.2236795   0.06037727 -0.07199733  0.15238186  0.01320335\n",
      "  0.09773192  0.11063358 -0.01586969 -0.12897211 -0.0201993   0.07721622\n",
      "  0.11132904 -0.13067909 -0.00924318  0.07260409  0.05481729 -0.05902149\n",
      " -0.2339786  -0.02057123  0.01306966 -0.13869832 -0.08822653 -0.07845674\n",
      " -0.32157976 -0.10953958  0.09557799 -0.06474664 -0.03778845  0.03837843\n",
      " -0.01723297 -0.0244724  -0.07212664  0.06304205 -0.05542718  0.08676886\n",
      " -0.19776228 -0.12506526 -0.03333469  0.05330176]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform basic functions on our standard zip/txt file\n",
    "to benchmark performance\n",
    "\"\"\"\n",
    "print('Standard Text File:')\n",
    "print('\\tFile Size: ', os.stat('data/glove/glove.6B.zip').st_size)\n",
    "\n",
    "start = time.time()\n",
    "glove_vectors_txt = glove_helper.Hands(ndim=100, quiet=True)\n",
    "end = time.time()\n",
    "print('\\tFile Load Time: ', end - start)\n",
    "\n",
    "start = time.time()\n",
    "glove_vectors_txt.get_vector('diabetes')\n",
    "glove_vectors_txt.get_vector('cancer')\n",
    "glove_vectors_txt.get_vector('hypertension')\n",
    "end = time.time()\n",
    "print('\\tQuery Time: ', end - start)\n",
    "\n",
    "print('\\tHandling out-of-vocabulary words:')\n",
    "try:\n",
    "    print('\\t\\t', glove_vectors_txt.get_vector('wordnotfoundinvocab'))\n",
    "except AssertionError:\n",
    "    print('\\t\\tWord not found in vocabulary')\n",
    "\n",
    "\"\"\"\n",
    "Perform basic functions on our magnitude file\n",
    "to benchmark performance\n",
    "\"\"\"\n",
    "print('\\nMagnitude File:')\n",
    "print('\\tFile Size: ', os.stat('data/glove-lemmatized.6B.100d.magnitude').st_size)\n",
    "\n",
    "start = time.time()\n",
    "glove_vectors_mag = Magnitude(\"data/glove-lemmatized.6B.100d.magnitude\")\n",
    "end = time.time()\n",
    "print('\\tFile Load Time: ', end - start)\n",
    "\n",
    "start = time.time()\n",
    "glove_vectors_mag.query(\"diabetes\")\n",
    "glove_vectors_mag.query(\"cancer\")\n",
    "glove_vectors_mag.query(\"hypertension\")\n",
    "end = time.time()\n",
    "print('\\tQuery Time: ', end - start)\n",
    "\n",
    "print('\\tHandling out-of-vocabulary words:')\n",
    "try:\n",
    "    print('\\t\\t', glove_vectors_mag.query('wordnotfoundinvocab'))\n",
    "except AssertionError:\n",
    "    print('\\t\\tWord not found in vocabulary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Selection - Biomedical Text\n",
    "\n",
    "With a framework that allows more freedom in corpus selection, we can move into much more larger word embedding models. The GloVe model we have been previously working with is actually on the smaller side. Of course, a larger corpus offers more data to train on, thus better capturing word contexts and meanings. However, another determininig factor in corpus selection is the source of the text. In general, these pre-trained models are based on general topic sources such as Wikipedia and Gigaword. However, since we know the domain we are working in, it may make sense to pull from relevant text sources. \n",
    "\n",
    "A Comparison of Word Embeddings for the Biomedical Natural Language Processing (https://arxiv.org/pdf/1802.00400.pdf) explores this idea. The paper concluded that \"word embeddings trained on EHR and MedLit can capture the semantics of medical terms better and find semantically relevant medical terms closer to human experts’ judgments than those trained on GloVe and Google News.\" \n",
    "\n",
    "We can test these results ourselves by comparing GloVe against a biomedical based word embedding that was trained on text from PubMed and PubMed Central."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe length:  336951\n",
      "GloVe dimensions:  100\n",
      "\n",
      "Nearest Neighbor examples:\n",
      "10 NN for diabetes:\n",
      " [('diabetic', 0.7566893059521317), ('diabetis', 0.7465886369685321), ('obesity', 0.619293770727618), ('hypertension', 0.6162751523182464), ('cardiovascular', 0.5791516470463346), ('asthma', 0.5689611839459698), ('arthriti', 0.5554265183541941), ('mellitu', 0.5439654800171492), ('allergy', 0.5393456576654493), ('alzheimer', 0.5297674264739546)]\n",
      "10 NN for cancer:\n",
      " [('breast', 0.8210739), ('prostate', 0.8065967), ('disease', 0.78536785), ('diabetis', 0.7788438), ('patient', 0.75117147), ('leukemia', 0.7485109), ('alzheimer', 0.744444), ('lung', 0.73171055), ('diseasis', 0.729254), ('heart', 0.7241202)]\n",
      "10 NN for hyperlipidemia:\n",
      " [('dyslipidemia', 0.6900931), ('hypercholesterolemia', 0.67991346), ('insulin-dependent', 0.6547221), ('insipidu', 0.61982), ('hyperglycemia', 0.6196113), ('vaginismu', 0.61709344), ('metformin', 0.6067523), ('pre-eclampsia', 0.60390294), ('prediabetis', 0.6029445), ('polymyositi', 0.60282224)]\n",
      "10 NN for e119:\n",
      " [('e19', 0.9162408356535976), ('phahon', 0.5892266446423353), ('werken', 0.5879736771166904), ('transsiberian', 0.5842905062903658), ('brookton', 0.5811409758690093), ('maai', 0.5788427014310997), ('surinaamse', 0.57560142232758), ('e22', 0.5739006580512447), ('50-2', 0.5728058762028345), ('bagaduce', 0.5709029514547086)]\n"
     ]
    }
   ],
   "source": [
    "print('GloVe length: ', len(glove_vectors_mag))\n",
    "print('GloVe dimensions: ', glove_vectors_mag.dim)\n",
    "\n",
    "print('\\nNearest Neighbor examples:')\n",
    "print('10 NN for diabetes:\\n', glove_vectors_mag.most_similar(\"diabetes\", topn = 10))\n",
    "print('10 NN for cancer:\\n', glove_vectors_mag.most_similar(\"cancer\", topn = 10))\n",
    "print('10 NN for hyperlipidemia:\\n', glove_vectors_mag.most_similar(\"hyperlipidemia\", topn = 10))\n",
    "print('10 NN for e119:\\n', glove_vectors_mag.most_similar(\"e119\", topn = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical length:  5443656\n",
      "Medical dimensions:  200\n",
      "\n",
      "Nearest Neighbor examples:\n",
      "10 NN for diabetes:\n",
      " [('T2DM', 0.849025), ('T1DM', 0.8185854), ('prediabetes', 0.8093643), ('mellitus', 0.803476), ('pre-diabetes', 0.78467065), ('DM2', 0.7815228), ('hyperlipidemia', 0.7732315), ('(IDDM)1', 0.763704), ('dyslipidemia', 0.7627928), ('hyperlipidaemia', 0.75711805)]\n",
      "10 NN for cancer:\n",
      " [('cancers', 0.85612094), ('caner', 0.8248045), ('CRC', 0.79751563), ('PCa', 0.7963365), ('cancer.4', 0.7522857), ('cancer.6', 0.7514417), ('cancer.5', 0.7498777), ('breast', 0.7488325), ('cancer.9', 0.74855363), ('cancer.10', 0.74781704)]\n",
      "10 NN for hyperlipidemia:\n",
      " [('hyperlipidaemia', 0.94658345), ('dyslipidemia', 0.92323184), ('dyslipidaemia', 0.90003157), ('hypercholesterolemia', 0.8709867), ('hypertriglyceridemia', 0.83977485), ('dislipidemia', 0.8359443), ('dyslipemia', 0.8347689), ('dyslipidemias', 0.83325243), ('hypertension', 0.80633545), ('dyslipoproteinemia', 0.7960845)]\n",
      "10 NN for e119:\n",
      " [('e19', 0.7128008916584654), ('E27', 0.6917508195017179), ('E19', 0.6650690460748763), ('E115', 0.6524077992099495), ('E11', 0.6444167702156325), ('E21', 0.6343371478110573), ('E37', 0.6333697905383757), ('E31', 0.6277420755217729), ('E42', 0.62765679930548), ('E78', 0.622213146286622)]\n"
     ]
    }
   ],
   "source": [
    "med_vectors = Magnitude(\"data/wikipedia-pubmed-and-PMC-w2v.magnitude\", pad_to_length=30)\n",
    "print('Medical length: ', len(med_vectors))\n",
    "print('Medical dimensions: ', med_vectors.dim)\n",
    "\n",
    "print('\\nNearest Neighbor examples:')\n",
    "print('10 NN for diabetes:\\n', med_vectors.most_similar(\"diabetes\", topn = 10))\n",
    "print('10 NN for cancer:\\n', med_vectors.most_similar(\"cancer\", topn = 10))\n",
    "print('10 NN for hyperlipidemia:\\n', med_vectors.most_similar(\"hyperlipidemia\", topn = 10))\n",
    "print('10 NN for e119:\\n', med_vectors.most_similar(\"e119\", topn = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data - Labeled Electronic Health Record Text\n",
    "\n",
    "The goal of this project is to classify Eletronic Health Record (EHR) text. This of course means that we need to get our hands on some EHR data. This can be particularly difficult due to the strict rules and guidelines around healthcare data. The Health Insurance Portability and Accountability Act of 1996, or HIPAA, outlines a set of rules that help protect the privacy of our health information. These rules are vital for building a healthcare system where we can trust our healthcare providers and caregivers, so it is important that we adhere to the standards set by HIPAA. \n",
    "\n",
    "For this project, we will be using a dataset provided by MTSamples.com. They provide ~5,000 transcribed medical reports covering 40 specialty types. All of the notes have been de-identified of protected health information, making them HIPAA compliant. Below we will explore a few rows of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EHR Sentence Example:\n",
      "\n",
      "['Bariatrics', 'PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.']\n",
      "['Bariatrics', 'HISTORY OF PRESENT ILLNESS: , I have seen ABC today.  He is a very pleasant gentleman who is 42 years old, 344 pounds.  He is 5\\'9\"\"\"\".  He has a BMI of 51.  He has been overweight for ten years since the age of 33, at his highest he was 358 pounds, at his lowest 260.  He is pursuing surgical attempts of weight loss to feel good, get healthy, and begin to exercise again.  He wants to be able to exercise and play volleyball.  Physically, he is sluggish.  He gets tired quickly.  He does not go out often.  When he loses weight he always regains it and he gains back more than he lost.  His biggest weight loss is 25 pounds and it was three months before he gained it back.  He did six months of not drinking alcohol and not taking in many calories.  He has been on multiple commercial weight loss programs including Slim Fast for one month one year ago and Atkin\\'s Diet for one month two years ago.,PAST MEDICAL HISTORY: , He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, difficulty walking, high cholesterol, and high blood pressure.  He has asthma and difficulty walking two blocks or going eight to ten steps.  He has sleep apnea and snoring.  He is a diabetic, on medication.  He has joint pain, knee pain, back pain, foot and ankle pain, leg and foot swelling.  He has hemorrhoids.,PAST SURGICAL HISTORY: , Includes orthopedic or knee surgery.,SOCIAL HISTORY: , He is currently single.  He drinks alcohol ten to twelve drinks a week, but does not drink five days a week and then will binge drink.  He smokes one and a half pack a day for 15 years, but he has recently stopped smoking for the past two weeks.,FAMILY HISTORY: , Obesity, heart disease, and diabetes.  Family history is negative for hypertension and stroke.,CURRENT MEDICATIONS:,  Include Diovan, Crestor, and Tricor.,MISCELLANEOUS/EATING HISTORY:  ,He says a couple of friends of his have had heart attacks and have had died.  He used to drink everyday, but stopped two years ago.  He now only drinks on weekends.  He is on his second week of Chantix, which is a medication to come off smoking completely.  Eating, he eats bad food.  He is single.  He eats things like bacon, eggs, and cheese, cheeseburgers, fast food, eats four times a day, seven in the morning, at noon, 9 p.m., and 2 a.m.  He currently weighs 344 pounds and 5\\'9\"\"\"\".  His ideal body weight is 160 pounds.  He is 184 pounds overweight.  If he lost 70% of his excess body weight that would be 129 pounds and that would get him down to 215.,REVIEW OF SYSTEMS: , Negative for head, neck, heart, lungs, GI, GU, orthopedic, or skin.  He also is positive for gout.  He denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, pulmonary embolism, or CVA.  He denies venous insufficiency or thrombophlebitis.  Denies shortness of breath, COPD, or emphysema.  Denies thyroid problems, hip pain, osteoarthritis, rheumatoid arthritis, GERD, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  He denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:  ,He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Neck is soft and supple.  Lungs:  He has positive wheezing bilaterally.  Heart is regular rhythm and rate.  His abdomen is soft.  Extremities:  He has 1+ pitting edema.,IMPRESSION/PLAN:,  I have explained to him the risks and potential complications of laparoscopic gastric bypass in detail and these include bleeding, infection, deep venous thrombosis, pulmonary embolism, leakage from the gastrojejuno-anastomosis, jejunojejuno-anastomosis, and possible bowel obstruction among other potential complications.  He understands.  He wants to proceed with workup and evaluation for laparoscopic Roux-en-Y gastric bypass.  He will need to get a letter of approval from Dr. XYZ.  He will need to see a nutritionist and mental health worker.  He will need an upper endoscopy by either Dr. XYZ.  He will need to go to Dr. XYZ as he previously had a sleep study.  We will need another sleep study.  He will need H. pylori testing, thyroid function tests, LFTs, glycosylated hemoglobin, and fasting blood sugar.  After this is performed, we will submit him for insurance approval.']\n"
     ]
    }
   ],
   "source": [
    "ehr_notes = []\n",
    "with open('data/ehr_samples.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ehr_notes.append([row['Specialty'], row['Note']])\n",
    "        \n",
    "print('EHR Sentence Example:\\n')\n",
    "print(ehr_notes[0])\n",
    "print(ehr_notes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing - Pre-Processing the EHR Notes\n",
    "\n",
    "With the EHR data now loaded, we could technically start applying Machine Learning operations as is. However, text can come in all forms and some models perform better when the input text is in a certain form. We discover a few of these optimizations throughout our ML notebooks, but it would be good to cover them here as well. \n",
    "\n",
    "The first obstacle is managing our text length. As our input text grows, so does the number of variables and the number of operations. Depending on our algorithm, these values can scale exponentially, causing runtime and resource usage to explode out of hand. To help manage the scope of our input text, we will be breaking up our notes into sentences. Limiting our text length also helps the model focus on the signals that matter. This should give us enough context to learn the more complex relationships between our words while minimizing runtime.\n",
    "\n",
    "Another pre-processing step we can take is to apply basic natural language cleanup techniques that standardize the text and remove non-essential information. Thankfully, python has a package called the Natural Language Toolkit (NLTK) that provides a lot of these transformations as built-in functions. The operations we will use for this project are converting all text to lowercase, removing punctation, filtering out stop words, and removing blanks.\n",
    "\n",
    "After all of the pre-processing, we can take a look at what the EHR notes now look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_sentences = []\n",
    "for record in ehr_notes:\n",
    "    sent_text = nltk.sent_tokenize(record[1])\n",
    "    for sent in sent_text:\n",
    "        tokens = word_tokenize(sent)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "#         # stem words\n",
    "#         porter = PorterStemmer()\n",
    "#         tokens = [porter.stem(word) for word in tokens]\n",
    "\n",
    "        # remove blanks\n",
    "        tokens = [w for w in tokens if w != '']\n",
    "\n",
    "        ehr_sentences.append([record[0], ' '.join(tokens)])\n",
    "\n",
    "random.Random(4).shuffle(ehr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Psychiatry', 'exhusband died 1980 acute pancreatitis secondary alcohol abuse'], ['Gynecology', 'patient taken post anesthesia care unit stable condition'], ['Consult', 'send pertussis pcr'], ['Discharge', 'admission diagnoses 1'], ['Surgery', 'time removed 12 mm broach proceeded implanting polyethylene liner within acetabulum'], ['General', 'peripheral vascular disease status post recent last week pta right lower extremity social history negative smoking drinking current home medications novolog 20 units meal lantus 30 units bedtime crestor 10 mg daily micardis 80 mg daily imdur 30 mg daily amlodipine 10 mg daily coreg 125 mg bid lasix 20 mg daily ecotrin 325 mg daily calcitriol 05 mcg daily review systems patient denies complaints states right hand left foot swollen painful came emergency room'], ['Surgery', 'estimated blood loss less 15 ml'], ['Surgery', 'base tumor fulgurated periphery normal mucosa surrounding base bladder tumor'], ['Cardiovascular', 'focal areas consolidation suggest pneumonia'], ['Surgery', 'enterogastritis procedure performed egd peg tube placement using russell technique anesthesia iv sedation 1 lidocaine local estimated blood loss none complications none brief history 44yearold africanamerican female well known service']]\n"
     ]
    }
   ],
   "source": [
    "print(ehr_sentences[:10])\n",
    "\n",
    "specialties = ['Allergy', 'Autopsy', 'Bariatrics', 'Cardiovascular', 'Chart', 'Chiropractic', 'Consult'\n",
    "               , 'Cosmetic', 'Dentistry', 'Dermatology', 'Diet', 'Discharge', 'Emergency', 'Endocrinology'\n",
    "               , 'Gastroenterology', 'General', 'Gynecology', 'Hospice', 'IME', 'Letters', 'Nephrology', 'Neurology'\n",
    "               , 'Neurosurgery', 'Office Notes', 'Oncology', 'Ophthalmology', 'Orthopedic', 'Otolaryngology'\n",
    "               , 'Pain Management', 'Pathology', 'Pediatrics', 'Podiatry', 'Psychiatry', 'Radiology', 'Rehab'\n",
    "               , 'Rheumatology', 'Sleep', 'Speech', 'Surgery', 'Urology']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODS AND APPROACHES\n",
    "## Naive Nearest Neighbor\n",
    "\n",
    "The first method we will explore will be to just leverage the word embedding space with no Machine Learning at all. We mentioned earlier that the word vectors capture context and meaning. Additionally position of these vectors in relation to eachother also convey word relationships. At the core of it, vectors clustered together are more similar in context and meaning. Using this principle, we can use our categories as anchors in our word embedding, calculate a similarity score for a sentence, and identify which category is the nearest neighbor to our sentence. \n",
    "\n",
    "This is a very naive approach but it will be a good exercise and can at least set a baseline for performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between diabetes and mellitus:  0.80347604\n",
      "Similarity between diabetes and breast:  0.26328182\n",
      "\n",
      "Similarity between cancer and mellitus:  0.13384798\n",
      "Similarity between cancer and breast:  0.7488326\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between diabetes and mellitus: ', med_vectors.similarity(\"diabetes\", \"mellitus\"))\n",
    "print('Similarity between diabetes and breast: ', med_vectors.similarity(\"diabetes\", \"breast\"))\n",
    "\n",
    "print('\\nSimilarity between cancer and mellitus: ', med_vectors.similarity(\"cancer\", \"mellitus\"))\n",
    "print('Similarity between cancer and breast: ', med_vectors.similarity(\"cancer\", \"breast\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Correct Classifications:  98\n",
      "Accuracy:  0.049\n"
     ]
    }
   ],
   "source": [
    "nn_results = []\n",
    "for i, ehr_sent in enumerate(ehr_sentences[0:2000]):\n",
    "#     print(ehr_sent)\n",
    "    \n",
    "    most_similar_specialty = []\n",
    "    \n",
    "    for specialty in specialties:\n",
    "        spec_similarity_sum = 0\n",
    "        for token in ehr_sent[1].split(' '):\n",
    "#             print('\\t', token, med_vectors.similarity(specialty, token))\n",
    "            \n",
    "            spec_similarity_sum += med_vectors.similarity(specialty, token)\n",
    "        \n",
    "        spec_similarity = spec_similarity_sum / len(ehr_sent[1].split(' '))\n",
    "        \n",
    "#         print(specialty, spec_similarity)\n",
    "\n",
    "        if not most_similar_specialty:\n",
    "            most_similar_specialty = [i, ehr_sent[0], specialty, spec_similarity]\n",
    "        elif spec_similarity > most_similar_specialty[3]:\n",
    "            most_similar_specialty = [i, ehr_sent[0], specialty, spec_similarity]\n",
    "        \n",
    "    nn_results.append(most_similar_specialty)\n",
    "\n",
    "    \n",
    "correct_results = [result for result in nn_results if result[1] == result[2]]\n",
    "print('# of Correct Classifications: ', len(correct_results))\n",
    "print('Accuracy: ', len(correct_results) / len(nn_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of correct classification:\n",
      "\tSentence:  ['Orthopedic', 'exposed vertebral bodies c2c3 c4c5 bridged plate']\n",
      "\n",
      "\tTrue category: Orthopedic\n",
      "\tPredicted category: Orthopedic\n",
      "\n",
      "\tSimilarities:\n",
      "\t\t exposed -0.03660483\n",
      "\t\t vertebral 0.266623\n",
      "\t\t bodies 0.068785824\n",
      "\t\t c2c3 0.031137193347711197\n",
      "\t\t c4c5 0.09221873311276046\n",
      "\t\t bridged 0.057441555\n",
      "\t\t plate 0.039083302\n",
      "\t\tAverage similarity:  0.1750587690063761\n"
     ]
    }
   ],
   "source": [
    "print('Example of correct classification:')\n",
    "\n",
    "correct_example = correct_results[0]\n",
    "example_sentence = ehr_sentences[correct_example[0]]\n",
    "print('\\tSentence: ', example_sentence)\n",
    "\n",
    "print('\\n\\tTrue category:', correct_example[1])\n",
    "print('\\tPredicted category:', correct_example[2])\n",
    "\n",
    "print('\\n\\tTrue/Predicted Similarities:')\n",
    "for token in example_sentence[1].split(' '):\n",
    "    print('\\t\\t', token, med_vectors.similarity(correct_example[1], token))\n",
    "    spec_similarity_sum += med_vectors.similarity(correct_example[1], token)\n",
    "\n",
    "spec_similarity = spec_similarity_sum / len(example_sentence.split(' '))\n",
    "print('\\t\\tAverage similarity: ', spec_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of incorrect classification:\n",
      "\tSentence:  ['Neurology', 'see velocity measurements left carotid eca measurement 0938 msecond']\n",
      "\n",
      "\tTrue category: Neurology\n",
      "\tPredicted category: Endocrinology\n",
      "\n",
      "\tTrue Similarities:\n",
      "\t\t see 0.034260437\n",
      "\t\t velocity 0.07255719\n",
      "\t\t measurements 0.015290075\n",
      "\t\t left 0.07127067\n",
      "\t\t carotid 0.07629804\n",
      "\t\t eca 0.094461195\n",
      "\t\t measurement -0.020491015\n",
      "\t\t 0938 0.10253124\n",
      "\t\t msecond 0.030581191\n",
      "\t\tAverage similarity:  0.48662400427592156\n",
      "\n",
      "\tPredicted Similarities:\n",
      "\t\t see -0.006658733\n",
      "\t\t velocity 0.05569666\n",
      "\t\t measurements 0.05710432\n",
      "\t\t left 0.05892444\n",
      "\t\t carotid 0.04782131\n",
      "\t\t eca 0.15068905\n",
      "\t\t measurement 0.05482881\n",
      "\t\t 0938 0.14380054\n",
      "\t\t msecond 0.061377887\n",
      "\t\tAverage similarity:  0.5559111470957501\n"
     ]
    }
   ],
   "source": [
    "print('Example of incorrect classification:')\n",
    "\n",
    "incorrect_example = nn_results[0]\n",
    "example_sentence = ehr_sentences[incorrect_example[0]]\n",
    "print('\\tSentence: ', example_sentence)\n",
    "\n",
    "print('\\n\\tTrue category:', incorrect_example[1])\n",
    "print('\\tPredicted category:', incorrect_example[2])\n",
    "\n",
    "print('\\n\\tTrue Similarities:')\n",
    "for token in example_sentence[1].split(' '):\n",
    "    print('\\t\\t', token, med_vectors.similarity(incorrect_example[1], token))\n",
    "    spec_similarity_sum += med_vectors.similarity(incorrect_example[1], token)\n",
    "\n",
    "spec_similarity = spec_similarity_sum / len(example_sentence[1].split(' '))\n",
    "print('\\t\\tAverage similarity: ', spec_similarity)\n",
    "\n",
    "print('\\n\\tPredicted Similarities:')\n",
    "for token in example_sentence[1].split(' '):\n",
    "    print('\\t\\t', token, med_vectors.similarity(incorrect_example[2], token))\n",
    "    spec_similarity_sum += med_vectors.similarity(incorrect_example[2], token)\n",
    "\n",
    "spec_similarity = spec_similarity_sum / len(example_sentence[1].split(' '))\n",
    "print('\\t\\tAverage similarity: ', spec_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see, the results are pretty terrible with an accuracy of 5%. Looking at an example the classifier got right, it relied on words that are exclusively and very distinctly related. However, these strong signals are not always present in our sentences. Looking at an incorrect example, we see how the signals are being drowned out or offset by the other words. This emphasizes the need for some type of model that can learn and weigh the words that provide strong signals for particular categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convultional Neural Network\n",
    "\n",
    "A neural network will allow us to build a model that can take in the word vectors as inputs and learn the complex relationships between those vectors to better classify the target sentence. This is a more holistic approach that tries to capture meaning from the entire sentence rather than token by token.\n",
    "\n",
    "In this project directory, you can find all the different iterations of CNNs that have been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Neural Network\n",
    "\n",
    "A LSTM is a type of neural network that tries to solve the problem of accurately capturing long term dependencies.\n",
    "\n",
    "In this project directory, you can find all the different iterations of LSTMs that have been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
