{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/renzeer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/renzeer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "import os, sys, re, json, time, wget, csv, string, time, random\n",
    "import itertools, collections\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GaussianNoise, Dropout, Dense, Embedding, MaxPool1D, GlobalMaxPool1D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pymagnitude import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_vectors = Magnitude(\"data/wikipedia-pubmed-and-PMC-w2v.magnitude\", pad_to_length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Other', 'PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.']\n",
      "['Cardiovascular', '2-D M-MODE: , ,1.  Left atrial enlargement with left atrial diameter of 4.7 cm.,2.  Normal size right and left ventricle.,3.  Normal LV systolic function with left ventricular ejection fraction of 51%.,4.  Normal LV diastolic function.,5.  No pericardial effusion.,6.  Normal morphology of aortic valve, mitral valve, tricuspid valve, and pulmonary valve.,7.  PA systolic pressure is 36 mmHg.,DOPPLER: , ,1.  Mild mitral and tricuspid regurgitation.,2.  Trace aortic and pulmonary regurgitation.']\n"
     ]
    }
   ],
   "source": [
    "ehr_notes = []\n",
    "with open('data/ehr_samples.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:     \n",
    "        if int(row['SpecialtyID']) in [39, 6, 16, 37, 11, 12, 29, 26, 7, 21, 19, 10, 2, 18]:\n",
    "            continue\n",
    "        elif int(row['SpecialtyID']) != 4:\n",
    "            ehr_notes.append(['Other', row['Note']])\n",
    "        else:\n",
    "            ehr_notes.append([row['Specialty'], row['Note']])\n",
    "\n",
    "print(ehr_notes[0])\n",
    "print(ehr_notes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cardiovascular', 'return clinic 4 weeks5']\n",
      "['Other', 'biceps tendon nonsubluxable']\n"
     ]
    }
   ],
   "source": [
    "ehr_sentences = []\n",
    "for record in ehr_notes:\n",
    "    sent_text = nltk.sent_tokenize(record[1])\n",
    "    for sent in sent_text:\n",
    "        tokens = word_tokenize(sent)\n",
    "\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "#         # stem words\n",
    "#         porter = PorterStemmer()\n",
    "#         tokens = [porter.stem(word) for word in tokens]\n",
    "\n",
    "        # remove blanks\n",
    "        tokens = [w for w in tokens if w != '']\n",
    "\n",
    "        ehr_sentences.append([record[0], ' '.join(tokens)])\n",
    "\n",
    "random.Random(4).shuffle(ehr_sentences)\n",
    "\n",
    "print(ehr_sentences[0])\n",
    "print(ehr_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Training and Test Data\n",
    "\n",
    "Before we can start building our neural networks, we first have to define our datasets. Specifically, we have to break up our EHR data so that we have records that we can train on and records that are exclusively used to test on. Maintaining a separate set for testing ensures we avoid overfitting our data.\n",
    "\n",
    "We will use some built-in functions provided by Magnitude that helps encode our classes/categories. We then partition our data into our train and test sets. For each set we have both data and labels. Initially, we will be making these partitions small to make iterating through model development much quicker. However, once the models are developed, we will expand our datasets to include all of our data. To ensure we defined our data correctly, we can print a few lines from the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70941"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ehr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardiovascular\n",
      "First line of train/test data:\n",
      "\t ['return', 'clinic', '4', 'weeks5']\n",
      "\t 0 Cardiovascular\n",
      "\t ['smokes', 'one', 'pack', 'per', 'day']\n",
      "\t 0 Cardiovascular\n",
      "Second line of train/test data:\n",
      "\t ['biceps', 'tendon', 'nonsubluxable']\n",
      "\t 1 Other\n",
      "\t ['denies', 'illicit', 'drug', 'use', 'family', 'history', 'parents', 'died', 'myocardial', 'infarctions']\n",
      "\t 1 Other\n"
     ]
    }
   ],
   "source": [
    "add_intent, intent_to_int, int_to_intent = MagnitudeUtils.class_encoding()\n",
    "\n",
    "x_train = [ehr_sent[1].split(' ') for ehr_sent in ehr_sentences[:60000]]\n",
    "x_test = [ehr_sent[1].split(' ') for ehr_sent in ehr_sentences[60001:]]\n",
    "\n",
    "y_train = [add_intent(ehr_sent[0]) for ehr_sent in ehr_sentences[:60000]]\n",
    "y_test = [add_intent(ehr_sent[0]) for ehr_sent in ehr_sentences[60001:]]\n",
    "\n",
    "y_train = list(np.array(y_train).reshape(len(y_train)))\n",
    "y_test = list(np.array(y_test).reshape(len(y_test)))\n",
    "\n",
    "num_training = len(x_train)\n",
    "num_test = len(x_test)\n",
    "num_outputs = int(max(max(y_train), max(y_test))) + 1\n",
    "\n",
    "print(int_to_intent(0))\n",
    "\n",
    "print(\"First line of train/test data:\")\n",
    "print(\"\\t\", x_train[0])\n",
    "print(\"\\t\", y_train[0], int_to_intent(y_train[0]))\n",
    "print(\"\\t\", x_test[0])\n",
    "print(\"\\t\", y_test[0], int_to_intent(y_test[0]))\n",
    "print(\"Second line of train/test data:\")\n",
    "print(\"\\t\", x_train[1])\n",
    "print(\"\\t\", y_train[1], int_to_intent(y_train[1]))\n",
    "print(\"\\t\", x_test[1])\n",
    "print(\"\\t\", y_test[1], int_to_intent(y_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_1 (GaussianNo (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 30, 128)           179328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 15, 128)           114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 302,530\n",
      "Trainable params: 302,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 30 # The maximum number of words the sequence model will consider\n",
    "STD_DEV = 0.01 # Deviation of noise for Gaussian Noise applied to the embeddings\n",
    "DROPOUT_RATIO = .5 # The ratio to dropout\n",
    "BATCH_SIZE = 100 # The number of examples per train/validation step\n",
    "EPOCHS = 100 # The number of times to repeat through all of the training data\n",
    "LEARNING_RATE = .01 # The learning rate for the optimizer\n",
    "NUM_FILTERS = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GaussianNoise(STD_DEV, input_shape=(MAX_WORDS, med_vectors.dim)))\n",
    "model.add(Conv1D(NUM_FILTERS, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(NUM_FILTERS, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(DROPOUT_RATIO))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_outputs, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 325s 542ms/step - loss: 0.3283 - acc: 0.8698 - val_loss: 0.3012 - val_acc: 0.8827\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.2771 - acc: 0.8909 - val_loss: 0.2823 - val_acc: 0.8857\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.2400 - acc: 0.9044 - val_loss: 0.2873 - val_acc: 0.8843\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.2012 - acc: 0.9173 - val_loss: 0.3143 - val_acc: 0.8782\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 35s 59ms/step - loss: 0.1766 - acc: 0.9253 - val_loss: 0.3276 - val_acc: 0.8676\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 36s 59ms/step - loss: 0.1593 - acc: 0.9321 - val_loss: 0.3730 - val_acc: 0.8505\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 37s 62ms/step - loss: 0.1441 - acc: 0.9387 - val_loss: 0.4094 - val_acc: 0.8604\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1295 - acc: 0.9444 - val_loss: 0.4658 - val_acc: 0.8702\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1225 - acc: 0.9465 - val_loss: 0.4472 - val_acc: 0.8625\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1170 - acc: 0.9490 - val_loss: 0.4278 - val_acc: 0.8336\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1111 - acc: 0.9520 - val_loss: 0.4506 - val_acc: 0.8596\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.1046 - acc: 0.9539 - val_loss: 0.4962 - val_acc: 0.8441\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1018 - acc: 0.9544 - val_loss: 0.4885 - val_acc: 0.8561\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.1006 - acc: 0.9550 - val_loss: 0.5115 - val_acc: 0.8664\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0971 - acc: 0.9558 - val_loss: 0.5694 - val_acc: 0.8628\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0956 - acc: 0.9564 - val_loss: 0.5512 - val_acc: 0.8548\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0879 - acc: 0.9583 - val_loss: 0.6144 - val_acc: 0.8671\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 36s 60ms/step - loss: 0.0886 - acc: 0.9576 - val_loss: 0.7091 - val_acc: 0.8671\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 36s 60ms/step - loss: 0.0866 - acc: 0.9578 - val_loss: 0.6653 - val_acc: 0.8687\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0845 - acc: 0.9580 - val_loss: 0.6614 - val_acc: 0.8663\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0843 - acc: 0.9586 - val_loss: 0.6955 - val_acc: 0.8679\n",
      "Epoch 24/100\n",
      "456/600 [=====================>........] - ETA: 8s - loss: 0.0822 - acc: 0.9594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0807 - acc: 0.9595 - val_loss: 0.6350 - val_acc: 0.8590\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0782 - acc: 0.9600 - val_loss: 0.6436 - val_acc: 0.8559\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0758 - acc: 0.9608 - val_loss: 0.6635 - val_acc: 0.8489\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0779 - acc: 0.9604 - val_loss: 0.6573 - val_acc: 0.8562\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0754 - acc: 0.9605 - val_loss: 0.7012 - val_acc: 0.8620\n",
      "Epoch 31/100\n",
      "469/600 [======================>.......] - ETA: 7s - loss: 0.0726 - acc: 0.9625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 37s 62ms/step - loss: 0.0737 - acc: 0.9617 - val_loss: 0.7328 - val_acc: 0.8622\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 37s 62ms/step - loss: 0.0720 - acc: 0.9613 - val_loss: 0.7432 - val_acc: 0.8581\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 35s 59ms/step - loss: 0.0724 - acc: 0.9617 - val_loss: 0.7661 - val_acc: 0.8612\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 36s 61ms/step - loss: 0.0715 - acc: 0.9629 - val_loss: 0.7335 - val_acc: 0.8649\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0694 - acc: 0.9625 - val_loss: 0.7892 - val_acc: 0.8676\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0701 - acc: 0.9626 - val_loss: 0.7734 - val_acc: 0.8644\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0674 - acc: 0.9634 - val_loss: 0.8142 - val_acc: 0.8658\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0682 - acc: 0.9634 - val_loss: 0.7832 - val_acc: 0.8690\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0700 - acc: 0.9633 - val_loss: 0.7585 - val_acc: 0.8668\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0665 - acc: 0.9631 - val_loss: 0.8355 - val_acc: 0.8698\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0677 - acc: 0.9633 - val_loss: 0.8180 - val_acc: 0.8654\n",
      "Epoch 44/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0662 - acc: 0.9637 - val_loss: 0.7847 - val_acc: 0.8706\n",
      "Epoch 45/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0661 - acc: 0.9641 - val_loss: 0.7997 - val_acc: 0.8690\n",
      "Epoch 46/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0663 - acc: 0.9642 - val_loss: 0.8201 - val_acc: 0.8655\n",
      "Epoch 47/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0649 - acc: 0.9642 - val_loss: 0.8683 - val_acc: 0.8664\n",
      "Epoch 48/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0655 - acc: 0.9635 - val_loss: 0.8469 - val_acc: 0.8640\n",
      "Epoch 49/100\n",
      "600/600 [==============================] - 37s 62ms/step - loss: 0.0650 - acc: 0.9641 - val_loss: 0.8493 - val_acc: 0.8627\n",
      "Epoch 50/100\n",
      "600/600 [==============================] - 35s 59ms/step - loss: 0.0646 - acc: 0.9648 - val_loss: 0.8363 - val_acc: 0.8638\n",
      "Epoch 51/100\n",
      "600/600 [==============================] - 37s 62ms/step - loss: 0.0653 - acc: 0.9633 - val_loss: 0.8209 - val_acc: 0.8628\n",
      "Epoch 52/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0640 - acc: 0.9645 - val_loss: 0.9131 - val_acc: 0.8669\n",
      "Epoch 53/100\n",
      "600/600 [==============================] - 40s 66ms/step - loss: 0.0634 - acc: 0.9646 - val_loss: 0.9188 - val_acc: 0.8671\n",
      "Epoch 54/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0635 - acc: 0.9650 - val_loss: 0.8240 - val_acc: 0.8700\n",
      "Epoch 55/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0622 - acc: 0.9654 - val_loss: 0.9099 - val_acc: 0.8693\n",
      "Epoch 56/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0623 - acc: 0.9652 - val_loss: 0.9203 - val_acc: 0.8611\n",
      "Epoch 57/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0639 - acc: 0.9644 - val_loss: 0.8304 - val_acc: 0.8604\n",
      "Epoch 58/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0632 - acc: 0.9642 - val_loss: 0.8914 - val_acc: 0.8648\n",
      "Epoch 59/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0644 - acc: 0.9643 - val_loss: 0.8507 - val_acc: 0.8652\n",
      "Epoch 60/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0611 - acc: 0.9649 - val_loss: 0.9406 - val_acc: 0.8674\n",
      "Epoch 61/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0628 - acc: 0.9649 - val_loss: 0.8785 - val_acc: 0.8658\n",
      "Epoch 62/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0606 - acc: 0.9654 - val_loss: 0.9624 - val_acc: 0.8674\n",
      "Epoch 63/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0624 - acc: 0.9657 - val_loss: 0.8365 - val_acc: 0.8660\n",
      "Epoch 64/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0621 - acc: 0.9651 - val_loss: 0.8861 - val_acc: 0.8661\n",
      "Epoch 65/100\n",
      "600/600 [==============================] - 36s 60ms/step - loss: 0.0605 - acc: 0.9660 - val_loss: 0.9813 - val_acc: 0.8702\n",
      "Epoch 66/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0609 - acc: 0.9669 - val_loss: 0.9499 - val_acc: 0.8662\n",
      "Epoch 67/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0613 - acc: 0.9654 - val_loss: 1.0598 - val_acc: 0.8665\n",
      "Epoch 68/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0608 - acc: 0.9656 - val_loss: 0.9117 - val_acc: 0.8656\n",
      "Epoch 69/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0602 - acc: 0.9653 - val_loss: 1.0313 - val_acc: 0.8656\n",
      "Epoch 70/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0612 - acc: 0.9653 - val_loss: 0.9972 - val_acc: 0.8674\n",
      "Epoch 71/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0598 - acc: 0.9654 - val_loss: 1.0231 - val_acc: 0.8650\n",
      "Epoch 72/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0592 - acc: 0.9656 - val_loss: 0.9943 - val_acc: 0.8661\n",
      "Epoch 73/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0599 - acc: 0.9653 - val_loss: 1.0538 - val_acc: 0.8652\n",
      "Epoch 74/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0593 - acc: 0.9663 - val_loss: 1.0331 - val_acc: 0.8645\n",
      "Epoch 75/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0598 - acc: 0.9651 - val_loss: 1.0186 - val_acc: 0.8658\n",
      "Epoch 76/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0589 - acc: 0.9657 - val_loss: 1.1095 - val_acc: 0.8684\n",
      "Epoch 77/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0594 - acc: 0.9655 - val_loss: 1.1079 - val_acc: 0.8683\n",
      "Epoch 78/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0593 - acc: 0.9659 - val_loss: 1.1167 - val_acc: 0.8685\n",
      "Epoch 79/100\n",
      "600/600 [==============================] - 37s 62ms/step - loss: 0.0583 - acc: 0.9658 - val_loss: 1.2302 - val_acc: 0.8701\n",
      "Epoch 80/100\n",
      "600/600 [==============================] - 36s 60ms/step - loss: 0.0596 - acc: 0.9653 - val_loss: 1.1560 - val_acc: 0.8616\n",
      "Epoch 81/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0600 - acc: 0.9656 - val_loss: 1.1542 - val_acc: 0.8674\n",
      "Epoch 82/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0587 - acc: 0.9656 - val_loss: 1.1036 - val_acc: 0.8686\n",
      "Epoch 83/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0566 - acc: 0.9671 - val_loss: 1.1554 - val_acc: 0.8696\n",
      "Epoch 84/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0601 - acc: 0.9656 - val_loss: 1.1415 - val_acc: 0.8678\n",
      "Epoch 85/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0574 - acc: 0.9659 - val_loss: 1.2571 - val_acc: 0.8669\n",
      "Epoch 86/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0591 - acc: 0.9669 - val_loss: 1.1363 - val_acc: 0.8680\n",
      "Epoch 87/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0579 - acc: 0.9659 - val_loss: 1.1524 - val_acc: 0.8662\n",
      "Epoch 88/100\n",
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0578 - acc: 0.9661 - val_loss: 1.1182 - val_acc: 0.8670\n",
      "Epoch 89/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0584 - acc: 0.9661 - val_loss: 1.0695 - val_acc: 0.8668\n",
      "Epoch 90/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0585 - acc: 0.9661 - val_loss: 1.0848 - val_acc: 0.8713\n",
      "Epoch 91/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0570 - acc: 0.9669 - val_loss: 1.1675 - val_acc: 0.8645\n",
      "Epoch 92/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0575 - acc: 0.9664 - val_loss: 1.1790 - val_acc: 0.8696\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 39s 66ms/step - loss: 0.0571 - acc: 0.9659 - val_loss: 1.1653 - val_acc: 0.8687\n",
      "Epoch 94/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0564 - acc: 0.9669 - val_loss: 1.1857 - val_acc: 0.8656\n",
      "Epoch 95/100\n",
      "600/600 [==============================] - 36s 60ms/step - loss: 0.0583 - acc: 0.9656 - val_loss: 1.1079 - val_acc: 0.8665\n",
      "Epoch 96/100\n",
      "600/600 [==============================] - 38s 63ms/step - loss: 0.0577 - acc: 0.9663 - val_loss: 1.1810 - val_acc: 0.8698\n",
      "Epoch 97/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0575 - acc: 0.9665 - val_loss: 1.1722 - val_acc: 0.8677\n",
      "Epoch 98/100\n",
      "600/600 [==============================] - 38s 64ms/step - loss: 0.0564 - acc: 0.9668 - val_loss: 1.1345 - val_acc: 0.8665\n",
      "Epoch 99/100\n",
      "600/600 [==============================] - 39s 64ms/step - loss: 0.0554 - acc: 0.9666 - val_loss: 1.1799 - val_acc: 0.8694\n",
      "Epoch 100/100\n",
      "600/600 [==============================] - 39s 65ms/step - loss: 0.0571 - acc: 0.9662 - val_loss: 1.1566 - val_acc: 0.8673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f610bd031d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_batches = MagnitudeUtils.batchify(x_train, y_train, BATCH_SIZE) # Split the training data into batches\n",
    "num_batches_per_epoch_train = int(np.ceil(num_training/float(BATCH_SIZE)))\n",
    "test_batches = MagnitudeUtils.batchify(x_test, y_test, BATCH_SIZE)  # Split the test data into batches\n",
    "num_batches_per_epoch_test = int(np.ceil(num_test/float(BATCH_SIZE)))\n",
    "\n",
    "\n",
    "# Generates batches of the transformed training data\n",
    "train_batch_generator = (\n",
    "  (\n",
    "    med_vectors.query(x_train_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_train_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for x_train_batch, y_train_batch in training_batches\n",
    ")\n",
    "\n",
    "# Generates batches of the transformed test data\n",
    "test_batch_generator = (\n",
    "  (\n",
    "    med_vectors.query(x_test_batch), # Magnitude will handle converting the 2D array of text into the 3D word vector representations!\n",
    "    MagnitudeUtils.to_categorical(y_test_batch, num_outputs) # Magnitude will handle converting the class labels into one-hot encodings!\n",
    "  ) for x_test_batch, y_test_batch in test_batches\n",
    ")\n",
    "\n",
    "# Start training\n",
    "from keras.utils import np_utils\n",
    "model.fit_generator(\n",
    "    generator = train_batch_generator,\n",
    "    steps_per_epoch = num_batches_per_epoch_train,\n",
    "    validation_data = test_batch_generator,\n",
    "    validation_steps = num_batches_per_epoch_test,\n",
    "    epochs = EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after training for 100 epochs:\n",
      "loss: 0.0506 - categorical_accuracy: 0.9680\n",
      "val_loss: 1.1566 - val_categorical_accuracy: 0.8673\n"
     ]
    }
   ],
   "source": [
    "print(\"Results after training for %d epochs:\" % (EPOCHS,))\n",
    "\n",
    "train_metrics = model.evaluate_generator(\n",
    "    generator = train_batch_generator,\n",
    "    steps = num_batches_per_epoch_train,\n",
    ")\n",
    "\n",
    "print(\"loss: %.4f - categorical_accuracy: %.4f\" % tuple(train_metrics))\n",
    "\n",
    "val_metrics = model.evaluate_generator(\n",
    "    generator = test_batch_generator,\n",
    "    steps = num_batches_per_epoch_test,\n",
    ")\n",
    "\n",
    "print(\"val_loss: %.4f - val_categorical_accuracy: %.4f\" % tuple(val_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
